{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "0d12d05ab1afc7278cbb1fa09e775842eb34e7aa5a193c340acf9f8f1f5bb7ed"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "(유형1) 풀이 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/mtcars.csv'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-c6ecc46b1a20>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/mtcars.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msubdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'qsec'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/mtcars.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('data/mtcars.csv')\n",
    "subdf = df['qsec']\n",
    "x = subdf.values.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "y=scaler.fit_transform(x)\n",
    "\n",
    "print((y>0.5).sum)\n"
   ]
  },
  {
   "source": [
    "(유형 1) 풀이 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas\n",
    "subdf = df.loc[:, 'qsec']\n",
    "subdf2  = df['qsec']\n",
    "x=subdf.values.reshape(-1,1)\n",
    "scaler = MinMaxScaler()\n",
    "y = scaler.fit_transform(x)\n",
    "record_num =[]\n",
    "for i in y:\n",
    "\tif i>0.5:\n",
    "\t\trecord_num.append(i)\n",
    "print(len(record_num))"
   ]
  },
  {
   "source": [
    "(유형 1) 풀이"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "(유형 2) 풀이 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer # 왜 썻을까?\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "path = 'C:/Users/KKK/Desktop/[Dataset] 작업형 제2유형/'\n",
    "X_train_path = path +'X_train.csv'\n",
    "X_test_path = path + 'X_test.csv'\n",
    "y_train_path = path + 'y_train.csv'\n",
    "y_test_path = path + '1234.csv'\n",
    "\n",
    "# Get Dataframe\n",
    "X_train = pd.read_csv(X_train_path, encoding='cp949')\n",
    "X_train = X_train.iloc[:, 1:]\n",
    "\n",
    "X_test = pd.read_csv(X_test_path , encoding='cp949')\n",
    "#cust id 먼저\n",
    "X_test_id = X_test.iloc[:,0]\n",
    "X_test = X_test.iloc[:, 1:]\n",
    "\n",
    "y_train = pd.read_csv(y_train_path)\n",
    "y_train = y_train.iloc[:,-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "      주구매상품_가공식품  주구매상품_가구  주구매상품_건강식품  ...  주구매상품_트래디셔널  주구매상품_피혁잡화  주구매상품_화장품\n0              0         0           0  ...            0           0          0\n1              0         0           0  ...            0           0          0\n2              0         0           0  ...            0           0          0\n3              0         0           0  ...            0           0          0\n4              0         0           0  ...            0           0          0\n...          ...       ...         ...  ...          ...         ...        ...\n3495           0         0           0  ...            0           0          0\n3496           0         0           0  ...            0           0          0\n3497           0         0           0  ...            0           0          0\n3498           0         0           0  ...            0           0          1\n3499           0         0           0  ...            0           0          0\n\n[3500 rows x 42 columns]\n"
     ]
    }
   ],
   "source": [
    "# LabelEncoder\n",
    "# X_train.loc[:, ['주구매상품','주구매지점']] = \\\n",
    "#     X_train.loc[:, ['주구매상품','주구매지점']].apply(LabelEncoder().fit_transform)\n",
    "# X_train.loc[:, ['환불금액']] = \\\n",
    "#     X_train.loc[:, ['환불금액']].fillna(0)\n",
    "\n",
    "# X_test.loc[:, ['주구매상품','주구매지점']] = \\\n",
    "#     X_test.loc[:, ['주구매상품','주구매지점']].apply(LabelEncoder().fit_transform)\n",
    "# X_test.loc[:, ['환불금액']] = \\\n",
    "#     X_test.loc[:, ['환불금액']].fillna(0)\n",
    "\n",
    "# OneHotEncoder\n",
    "\n",
    "# X_train = pd.get_dummies( X_train.loc[:, ['주구매상품']])\n"
   ]
  },
  {
   "source": [
    "IQR 이상치 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          총구매액     최대구매액       환불금액  주구매상품  ...  내점일수   내점당구매건수    주말방문비율  구매주기\n0     68282840  11264000  6860000.0      5  ...    19  3.894737  0.527027    17\n1      2136000   2136000   300000.0     21  ...     2  1.500000  0.000000     1\n2      3197000   1639000        0.0      6  ...     2  2.000000  0.000000     1\n3     16077620   4935000        0.0      5  ...    18  2.444444  0.318182    16\n4     29050000  24000000        0.0     15  ...     2  1.500000  0.000000    85\n...        ...       ...        ...    ...  ...   ...       ...       ...   ...\n3494  95835000  81650000        0.0     17  ...     7  1.285714  0.111111    34\n3495   3175200   3042900        0.0      3  ...     1  2.000000  1.000000     0\n3496  29628600   7200000  6049600.0     22  ...     8  1.625000  0.461538    40\n3497     75000     75000        0.0     32  ...     1  1.000000  0.000000     0\n3498   1875000   1000000        0.0     41  ...     2  1.000000  0.000000    39\n\n[3144 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "# def get_outlier(df=None, column=None, weight=1.5):\n",
    "#   # target 값과 상관관계가 높은 열을 우선적으로 진행\n",
    "#   quantile_25 = np.percentile(df[column].values, 25)\n",
    "#   quantile_75 = np.percentile(df[column].values, 75)\n",
    "\n",
    "#   IQR = quantile_75 - quantile_25\n",
    "#   IQR_weight = IQR*weight\n",
    "  \n",
    "#   lowest = quantile_25 - IQR_weight\n",
    "#   highest = quantile_75 + IQR_weight\n",
    "  \n",
    "#   outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index\n",
    "#   return outlier_idx\n",
    "\n",
    "# outlier_idx = get_outlier(df=X_train, column='총구매액', weight=1.5)\n",
    "# X_train.drop(outlier_idx, axis=0, inplace=True)\n",
    "# print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LR ROCAUC Score:  0.6352677527639534\n"
     ]
    }
   ],
   "source": [
    "# LR\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print('LR ROCAUC Score: ', roc_auc_score(y_train, pd.DataFrame(model.predict_proba(X_train)).iloc[:,1]))\n",
    "\n",
    "predict = model.predict_proba(X_test)\n",
    "predict = pd.DataFrame(predict)\n",
    "predict =predict.iloc[:, 1]\n",
    "\n",
    "# LR Model Predict\n",
    "answer = pd.concat([X_test_id, predict], axis = 1)\n",
    "answer.to_csv(y_test_path, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NN ROCAUC Score 0.576493731698899\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (10, ), solver = 'adam', activation = 'relu', learning_rate_init = 0.001, max_iter = 500)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "print('MLP ROCAUC Score', roc_auc_score(y_train, pd.DataFrame(mlp.predict_proba(X_train)).iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NN ROCAUC Score 0.7386293101528664\n"
     ]
    }
   ],
   "source": [
    "#RF\n",
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 5, random_state = 999)\n",
    "rf.fit(X_train, y_train)\n",
    "print('RF ROCAUC Score', roc_auc_score(y_train, pd.DataFrame(rf.predict_proba(X_train)).iloc[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "KNN ROCAUC SCORE 0.78464579366935\n0       0.0\n1       0.2\n2       0.2\n3       0.4\n4       0.0\n       ... \n2477    0.8\n2478    0.4\n2479    0.4\n2480    0.2\n2481    1.0\nName: 1, Length: 2482, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# kNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "print('KNN ROCAUC SCORE', roc_auc_score(y_train, pd.DataFrame(knn.predict_proba(X_train)).iloc[:,1]))\n",
    "\n",
    "predict = pd.DataFrame(knn.predict_proba(X_test)).iloc[:,1]\n",
    "print(predict)\n",
    "\n",
    "answer = pd.concat([X_test_id, predict], axis = 1)\n",
    "answer.to_csv(y_test_path, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  }
 ]
}